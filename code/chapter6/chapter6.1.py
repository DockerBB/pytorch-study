# 6.1 语言模型

# 6.1 语言模型的计算
# 假设序列w1,w2,...,wT中的每个词是依次生成的，我们有
# P(w1,w2,...,wT) = Π(t=1,T)P(wt|w1,...,w(t-1))
# 例如，一段含有4个词的文本序列的概率
# P(w1,w2,w3,w4) = P(w1)P(w2|w1)P(w3|w1,w2)P(w4|w1,w2,w3)
# 为了计算语言模型，我们需要计算词在给定前几个词的情况下的条件概率，
# 即语言模型参数。设训练数据集为一个大型文本语料库，如维基百科的所有
# 条目。词的概率可以通过该词在训练集的相对词频来计算。例如，P(w1)可以
# 计算为w1在训练集中的词频(词出现的次数)与训练集的总词数之比。因此，
# 根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过
# 训练数据集中的相对词频计算。例如，P(w2|w1)可以计算为w1,w2两次相邻的
# 频率与w1词频的比值，因为该比值即P(w1,w2)与P(w1)之比；而P(w3|w1,w2)
# 同理可以计算为w1,w2,w3三词相邻的频率与w1,w2两词相邻的频率的比值。以
# 此类推。

# n元语法
# 当序列长度增加时,计算和存储多个词共同出现的概率的复杂度会呈指数级
# 增加。n元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的
# 计算。这里的马尔可夫假设是指一个词的出现只与前面n个词相关,即n阶马
# 尔可夫链（Markov chain of order n）。如果n=1，那么有P(w3|w1,w2)=
# P(w3|w2)。如果基于n−1阶马尔可夫链，我们可以将语言模型改写为
# P(w1,w2,...,wT) ≈ Π(t=1,T)P(wt|w(t-(n-1)),...,w(t-1))
# 以上也叫n元语法（n-grams）。它是基于n - 1n−1阶马尔可夫链的概率语
# 言模型。当n分别为1、2和3时，我们将其分别称作一元语法（unigram）、
# 二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列w1,
# w2, w3, w4在一元语法、二元语法和三元语法中的概率分别为
# P(w1,w2,w3,w4) = P(w1)P(w2)P(w3)P(w4)
# P(w1,w2,w3,w4) = P(w1)P(w2|w1)P(w3|w2)P(w4|w3)
# P(w1,w2,w3,w4) = P(w1)P(w2|w1)P(w3|w1,w2)P(w4|w2,w3)
# 当n较小时，nn元语法往往并不准确。例如，在一元语法中，由三个词组成
# 的句子“你走先”和“你先走”的概率是一样的。然而，当n较大时，n元
# 语法需要计算并存储大量的词频和多词相邻频率。